model:
  organization: "HuggingFaceH4"
  name: "zephyr-7b-beta"
init:
  s3:
    enabled: true
    bucketURL: s3://k8s-model-zephyr/llm/deployment/zephyr-7b-beta

huggingface:
  containerPort: 8080
  args:
    - "--max-total-tokens"
    - "4048"
    - "--max-input-length"
    - "3096"
    #- --quantize
    #- "awq"
    #- "--num-shard"
    #- "1"
replicaCount: 1
kind: Deployment
updateStrategy:
  type: Recreate
image:
  repo: ghcr.io/huggingface/text-generation-inference
  tag: "latest"
  pullPolicy: IfNotPresent

persistence:
  accessModes:
  - ReadWriteOnce
  storageClassName: gp2
  storage: 100Gi

resources:
  #requests:
  #  cpu: "3"
  #  memory: "10Gi"
  limits:
    nvidia.com/gpu: 1

nodeSelector: {}
tolerations: []
