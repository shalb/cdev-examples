model:
  organization: "HuggingFaceH4"
  name: "zephyr-7b-beta"
init:
  s3:
    enabled: true
    bucketURL: s3://k8s-model-zephyr/llm/deployment/zephyr-7b-beta/

huggingface:
  containerPort: 8080
  args:
    - "--quantize"
    - "bitsandbytes"
    - "--num-shard"
    - "1"

replicaCount: 1
kind: deployment

image:
  repo: ghcr.io/huggingface/text-generation-inference
  tag: "latest"
  pullPolicy: IfNotPresent

persistence:
  accessModes:
  - ReadWriteOnce
  storageClassName: gp2
  storage: 100Gi

service:
  port: 8080
  type: "ClusterIP"

serviceAccount:
  create: true

resources:
  requests:
    cpu: "3"
    memory: "10Gi"
  limits:
    nvidia.com/gpu: 1

nodeSelector: {}
tolerations: []