model:
  organization: "WizardLM"
  name: "WizardCoder-Python-13B-V1.0"
init:
  s3:
    enabled: false
    bucketURL: s3://k8s-model-zephyr/llm/deployment/wizard-7b-uncensored

huggingface:
  containerPort: 8080
  args:
    - "--max-total-tokens"
    - "4048"
    - "--max-input-length"
    - "3096"
    #- --quantize
    #- "awq"
    #- "--num-shard"
    #- "1"
replicaCount: 1
kind: Deployment

image:
  repo: ghcr.io/huggingface/text-generation-inference
  tag: "latest"
  pullPolicy: IfNotPresent

persistence:
  accessModes:
  - ReadWriteOnce
  storageClassName: gp2
  storage: 100Gi

service:
  port: 8080
  type: "ClusterIP"

serviceAccount:
  create: false

resources:
  #requests:
  #  cpu: "3"
  #  memory: "10Gi"
  limits:
    nvidia.com/gpu: 1

nodeSelector: {}
tolerations: []
