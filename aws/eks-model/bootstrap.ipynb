{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Kubernetes cluster and deploy HF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- git-lfs installed to clone repo to s3\n",
    "- 100 GB of free space on local disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing model files\n",
    "We would download model locally and then move it files to aws s3 bucket to be mounted by model container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting required env variables\n",
    "\n",
    "%env S3_BUCKET_NAME=k8s-model-zephyr\n",
    "%env REGION=eu-central-1\n",
    "%env HF_MODEL_PATH=HuggingFaceH4/zephyr-7b-beta\n",
    "%env HF_MODEL_NAME=zephyr-7b-beta\n",
    "%env LOCAL_DIRECTORY=/data-tst/home/voa/projects/k8s-model\n",
    "%env AWS_PROFILE voatsap-cluster-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone model to local folder and upload to s3 bucket\n",
    "# this takes in my env(gigabit internet connection) ~9 min for clone and 6 min to upload\n",
    "\n",
    "!mkdir $LOCAL_DIRECTORY/$HF_MODEL_NAME\n",
    "!git lfs clone --depth=1 https://huggingface.co/$HF_MODEL_PATH $LOCAL_DIRECTORY/$HF_MODEL_NAME\n",
    "!aws s3 mb s3://$S3_BUCKET_NAME --region $REGION || true\n",
    "!aws s3 sync $LOCAL_DIRECTORY/$HF_MODEL_NAME s3://$S3_BUCKET_NAME/llm/deployment/$HF_MODEL_NAME --exclude \"*.git/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of HF model url in s3 bucket\n",
    "\n",
    "!echo s3://$S3_BUCKET_NAME/llm/deployment/$HF_MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing cluster.dev stack variables\n",
    "In cluster.dev folder there are 4 files:\n",
    "- `project.yaml` to define some global variables like region\n",
    "- `backend.yaml` required to set some state s3 bucket for cluster.dev and TF states\n",
    "- `stack-eks.yaml` file describing values for EKS cluster configuration with required node groups with GPU support, GPU types\n",
    "- `stack-model.yaml` Model variables required to deploy into EKS cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap cluster\n",
    "!cd cluster.dev\n",
    "!cdev apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to export KUBECONFIG to use kubectl\n",
    "!export KUBECONFIG=`pwd`/kubeconfig\n",
    "# Then we can examine workloads deployed in `default` namespace, since we define it in stack-model.yaml\n",
    "!kubectl get pod\n",
    "# To get logs from model startup, check if model is loaded without errors\n",
    "!kubectl logs -f <output model pod name from kubectl get pod>\n",
    "# To list services (should be model, chat and mongo if chat enabled)\n",
    "!kubectl get svc\n",
    "# Then you can port-forward service to your host\n",
    "!kubectl port-forward svc/<model-output from above>  8080:8080\n",
    "# Now you can chat with your model\n",
    "!curl 127.0.0.1:8080/generate \\\n",
    "    -X POST \\\n",
    "    -d '{\"inputs\":\"Continue funny story: John decide to stick finger into outlet\",\"parameters\":{\"max_new_tokens\":1000}}' \\\n",
    "    -H 'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring\n",
    "\n",
    "Ref: https://aws.amazon.com/blogs/machine-learning/enable-pod-based-gpu-metrics-in-amazon-cloudwatch/\n",
    "\n",
    "Now it could be set manually, but we'll add a monitoring stack to stackTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl https://raw.githubusercontent.com/NVIDIA/dcgm-exporter/main/etc/dcp-metrics-included.csv > /tmp/dcgm-metrics.csv\n",
    "\n",
    "kubectl create namespace gpu-operator\n",
    "kubectl create configmap metrics-config -n gpu-operator --from-file=/tmp/dcgm-metrics.csv\n",
    "\n",
    "helm install --wait --generate-name -n gpu-operator --create-namespace nvidia/gpu-operator \\\n",
    "--set dcgmExporter.config.name=metrics-config \\\n",
    "--set toolkit.enabled=false\n",
    "\n",
    "# Install prometheus stack\n",
    "helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n",
    "helm inspect values prometheus-community/kube-prometheus-stack > /tmp/kube-prometheus-stack.values\n",
    "\n",
    "sed -i '/serviceMonitorSelectorNilUsesHelmValues/ s/true/false/' /tmp/kube-prometheus-stack.values\n",
    "yq eval '.prometheus.prometheusSpec.additionalScrapeConfigs += [{\"job_name\": \"gpu-metrics\", \"scrape_interval\": \"1s\", \"metrics_path\": \"/metrics\", \"scheme\": \"http\", \"kubernetes_sd_configs\": [{\"role\": \"endpoints\", \"namespaces\": {\"names\": [\"gpu-operator\"]}}], \"relabel_configs\": [{\"source_labels\": [\"__meta_kubernetes_pod_node_name\"], \"action\": \"replace\", \"target_label\": \"kubernetes_node\"}]}]' /tmp/kube-prometheus-stack.values -i\n",
    "\n",
    "# get admin password for Grafana\n",
    "kubectl -n prometheus get secret $(kubectl -n prometheus get secrets | grep grafana | cut -d ' ' -f 1) -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n",
    "\n",
    "# port forward Grafana\n",
    "kubectl port-forward -n prometheus svc/$(kubectl -n prometheus get svc | grep grafana | cut -d ' ' -f 1) 8080:80 &\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
